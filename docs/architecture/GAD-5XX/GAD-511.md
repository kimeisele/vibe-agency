# GAD-511: Neural Adapter / Provider Strategy

**Version:** 1.0  
**Status:** ✅ Implemented  
**Locations:**
- `agency_os/00_system/runtime/llm_client.py` (LLMClient adapter)
- `agency_os/00_system/runtime/providers/` (provider implementations)

---

## Executive Summary

Enables **multi-provider LLM support** with graceful failover using the **Strategy Pattern**. The system works with:

- **Anthropic** (Claude 3.5 Sonnet) — Production ready
- **Google** (Gemini 2.5/2.0/1.5 Flash/Pro) — Production ready
- **OpenAI** (Future Phase 2)
- **Local/Ollama** (Future Phase 2)
- **NoOp** (Fallback mock provider)

**Key Insight:** One environment variable switches providers; missing API keys don't crash the system.

---

## Problem Statement

Without multi-provider support:

1. **Single point of failure** — One provider outage breaks the system
2. **Vendor lock-in** — Switching providers requires code changes
3. **No graceful degradation** — Missing API key crashes system
4. **Cost opacity** — Different providers have different pricing models

**Example:** Anthropic API down → System crashes. No way to failover to Google without code change.

---

## Solution: Strategy Pattern

The `LLMProvider` abstract base class defines the contract:

```python
class LLMProvider(ABC):
    @abstractmethod
    def invoke(self, prompt, model, max_tokens, temperature, **kwargs) -> LLMResponse:
        """Invoke LLM - provider-specific implementation"""
    
    @abstractmethod
    def calculate_cost(self, input_tokens, output_tokens, model) -> float:
        """Calculate cost - provider-specific pricing"""
    
    @abstractmethod
    def get_available_models(self) -> list[str]:
        """List available models for this provider"""
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if provider is usable"""
    
    def get_provider_name(self) -> str:
        """Human-readable name"""
```

**Benefits:**
- All providers implement same interface
- System code doesn't know provider details
- Easy to add new providers
- Uniform error handling

---

## Supported Providers

### 1. AnthropicProvider

**Location:** `providers/anthropic.py`

**Models:**
- `claude-3-5-sonnet-20241022`
- `claude-3-5-sonnet-20250129`
- `claude-3-5-sonnet-latest`

**Pricing (as of 2025-11-19):**
- Input: $3.00 per million tokens
- Output: $15.00 per million tokens

**Features:**
- Retry with exponential backoff (2s, 4s, 8s)
- Retryable errors: RateLimitError, APIConnectionError, APITimeoutError
- Full cost tracking

**Status:** ✅ Production ready

**Example:**
```python
from agency_os.00_system.runtime.providers import AnthropicProvider

provider = AnthropicProvider(api_key="sk-ant-...")

response = provider.invoke(
    prompt="What is 2+2?",
    model="claude-3-5-sonnet-20241022",
    max_tokens=100,
)

print(f"Cost: ${response.usage.cost_usd:.4f}")
```

### 2. GoogleProvider

**Location:** `providers/google.py`

**Models:**
- `gemini-2.5-flash-exp` (Free, experimental, latest)
- `gemini-2.0-flash-exp` (Free, experimental)
- `gemini-1.5-flash` (Stable, $0.075/$0.30)
- `gemini-1.5-flash-latest`
- `gemini-1.5-pro` (Professional, $1.25/$5.00)
- `gemini-1.5-pro-latest`

**Pricing (as of 2025-11-19):**
- Gemini 2.5/2.0 (preview): FREE
- Gemini 1.5 Flash: $0.075 input, $0.30 output
- Gemini 1.5 Pro: $1.25 input, $5.00 output

**Features:**
- Retry with exponential backoff
- Retryable errors: ResourceExhausted, ServiceUnavailable, DeadlineExceeded
- Token metadata extraction
- Best-effort token counting

**Status:** ✅ Production ready

**Example:**
```python
from agency_os.00_system.runtime.providers import GoogleProvider

provider = GoogleProvider(api_key="...")

response = provider.invoke(
    prompt="Explain photosynthesis",
    model="gemini-2.5-flash-exp",
    max_tokens=500,
)

print(f"Cost: ${response.usage.cost_usd:.4f}")  # Free!
```

### 3. NoOpProvider

**Location:** `providers/base.py`

**Model:** `noop` (mock)

**Pricing:** FREE ($0.00)

**Features:**
- Always available (no API key needed)
- Returns empty JSON response `{}`
- Zero cost
- Graceful fallback when no provider available

**Status:** ✅ Always available (fallback)

**Use Cases:**
- Development without API keys
- Testing
- Fallback when other providers unavailable

**Example:**
```python
from agency_os.00_system.runtime.providers import NoOpProvider

provider = NoOpProvider()

response = provider.invoke(
    prompt="Test",
    model="noop",
)

print(response.content)  # Output: "{}"
print(response.usage.cost_usd)  # Output: 0.0
```

### Future Providers (Phase 2)

- **OpenAIProvider** — GPT-4, GPT-3.5
- **LocalProvider** — Ollama, vLLM (self-hosted)

---

## Provider Selection & Auto-Detection

### Factory Pattern

**Location:** `providers/factory.py`

```python
def get_default_provider() -> LLMProvider:
    """Auto-detect and create default provider"""
    provider_name = _detect_provider()
    api_key = _get_api_key_for_provider(provider_name)
    return create_provider(provider_name, api_key)

def create_provider(
    provider_name: str | None = None,
    api_key: str | None = None,
) -> LLMProvider:
    """Create provider by name"""
```

### Auto-Detection Priority

The factory automatically detects which provider to use:

```python
def _detect_provider() -> str:
    """
    Detection priority (first available wins):
    1. GOOGLE_API_KEY → google
    2. ANTHROPIC_API_KEY → anthropic
    3. OPENAI_API_KEY → openai
    4. None available → noop (graceful fallback)
    """
```

**Search Order:**
1. Check `GOOGLE_API_KEY` (highest priority)
2. Check `ANTHROPIC_API_KEY`
3. Check `OPENAI_API_KEY`
4. Fall back to `NoOpProvider`

### API Key Validation

```python
def is_valid_key(key: str | None) -> bool:
    """
    Check if key is valid:
    - Not None
    - Not empty
    - Not a placeholder
    """
    if not key:
        return False
    
    placeholders = ["your-", "xxx", "placeholder", "example", "test-key"]
    return not any(ph in key.lower() for ph in placeholders)
```

**Filters out:** `your-api-key`, `test-key-123`, placeholder values.

---

## Graceful Failover Strategy

### Failover Chain

```
Try GOOGLE_API_KEY
  ├─ Success → Use GoogleProvider
  └─ Fail → Try ANTHROPIC_API_KEY
      ├─ Success → Use AnthropicProvider
      └─ Fail → Try OPENAI_API_KEY
          ├─ Success → Use OpenAIProvider (Phase 2)
          └─ Fail → Use NoOpProvider (safe fallback)
```

### What "Graceful" Means

1. **No crashes:** If no valid API key found → NoOpProvider (mock mode)
2. **Mock responses:** Returns empty JSON with zero cost
3. **System continues:** Development/testing works without real API keys
4. **Clear logging:** Warns developers that mock mode is active

**Example:**
```bash
$ python script.py
# No ANTHROPIC_API_KEY, OPENAI_API_KEY, or GOOGLE_API_KEY set

# Logs:
# INFO: No API keys detected. Activating Mock/Offline Mode (NoOp provider)

# Script continues, using mock provider
```

---

## Missing API Key Scenarios

### Scenario 1: No API Keys Set

```bash
$ ANTHROPIC_API_KEY="" GOOGLE_API_KEY="" python script.py
```

**Result:**
- Factory detects no valid keys
- Falls back to `NoOpProvider`
- `LLMClient.mode = "noop"`
- All invocations return mock responses

**Logs:**
```
INFO: No API keys detected. Activating Mock/Offline Mode (NoOp provider)
WARNING: NoOpProvider initialized - running in mock mode
```

### Scenario 2: Invalid/Placeholder Key

```bash
$ ANTHROPIC_API_KEY="your-api-key-here" python script.py
```

**Result:**
- Factory detects placeholder value (contains "your-")
- Skips Anthropic
- Tries next provider (Google)
- Falls through to NoOpProvider if none valid

**Logs:**
```
DEBUG: Filtering out placeholder value "your-api-key-here"
```

### Scenario 3: Key Set but Library Missing

```bash
$ ANTHROPIC_API_KEY="sk-ant-..." python script.py
# But `pip install anthropic` not done
```

**Result:**
- AnthropicProvider initialization fails with `ImportError`
- Factory catches exception
- Falls back to NoOpProvider

**Logs:**
```
ERROR: anthropic package not installed. 
Install with: pip install anthropic>=0.18.0
WARNING: Provider anthropic not available, using NoOp fallback
```

### Scenario 4: API Key Valid but API Down

```bash
$ ANTHROPIC_API_KEY="sk-ant-..." python script.py
# Anthropic API is down
```

**Result:**
- AnthropicProvider initialized successfully
- `invoke()` makes API call
- API call fails (connection error, etc.)
- Raises `ProviderInvocationError` (not caught)
- Circuit breaker (GAD-509) catches and opens circuit

**Protected by:** GAD-509 Circuit Breaker

---

## Provider Interface

### Standardized Response

All providers return the same `LLMResponse` structure:

```python
@dataclass
class LLMResponse:
    content: str              # The response text
    usage: LLMUsage           # Token counts and cost
    model: str                # Model that was used
    finish_reason: str        # "stop", "no_provider", etc.
    provider: str             # "anthropic", "google", "noop"

@dataclass
class LLMUsage:
    input_tokens: int         # Prompt tokens
    output_tokens: int        # Completion tokens
    model: str                # Model identifier
    cost_usd: float           # Calculated cost
    timestamp: str            # ISO 8601 with Z
```

### Standardized Exceptions

```python
class LLMProviderError(Exception):
    """Base exception for provider errors"""

class ProviderNotAvailableError(LLMProviderError):
    """Provider cannot be initialized (missing API key, library, etc.)"""

class ProviderInvocationError(LLMProviderError):
    """Provider invocation failed after retries"""
```

---

## Provider Implementations

### AnthropicProvider

```python
class AnthropicProvider(LLMProvider):
    PRICING = {
        "claude-3-5-sonnet-20241022": {"input": 3.0, "output": 15.0},
        "claude-3-5-sonnet-20250129": {"input": 3.0, "output": 15.0},
    }
    
    def __init__(self, api_key: str | None = None, **kwargs):
        if not api_key:
            raise ProviderNotAvailableError("API key required")
        
        from anthropic import Anthropic
        self.client = Anthropic(api_key=api_key)
    
    def invoke(self, prompt, model, max_tokens, temperature, max_retries=3):
        """Invoke Claude with retry logic"""
        for attempt in range(max_retries):
            try:
                response = self.client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    messages=[{"role": "user", "content": prompt}],
                )
                
                cost = self.calculate_cost(
                    response.usage.input_tokens,
                    response.usage.output_tokens,
                    model,
                )
                
                return LLMResponse(
                    content=response.content[0].text,
                    usage=LLMUsage(...),
                    model=response.model,
                    finish_reason=response.stop_reason,
                    provider="anthropic",
                )
            
            except Exception as e:
                # Retry on rate limit / timeout / connection errors
                if is_retryable(e) and attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    raise ProviderInvocationError(...)
```

### GoogleProvider

```python
class GoogleProvider(LLMProvider):
    PRICING = {
        "gemini-2.5-flash-exp": {"input": 0.0, "output": 0.0},
        "gemini-1.5-flash": {"input": 0.075, "output": 0.30},
        "gemini-1.5-pro": {"input": 1.25, "output": 5.00},
    }
    
    def __init__(self, api_key: str | None = None, **kwargs):
        if not api_key:
            raise ProviderNotAvailableError("API key required")
        
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        self.genai = genai
    
    def invoke(self, prompt, model="gemini-2.5-flash-exp", ...):
        """Invoke Gemini with retry logic"""
        for attempt in range(max_retries):
            try:
                gemini_model = self.genai.GenerativeModel(model)
                response = gemini_model.generate_content(
                    prompt,
                    generation_config={
                        "max_output_tokens": max_tokens,
                        "temperature": temperature,
                    },
                )
                
                # Extract token counts
                input_tokens = response.usage_metadata.prompt_token_count
                output_tokens = response.usage_metadata.candidates_token_count
                
                cost = self.calculate_cost(input_tokens, output_tokens, model)
                
                return LLMResponse(
                    content=response.text,
                    usage=LLMUsage(...),
                    model=model,
                    finish_reason=str(response.candidates[0].finish_reason),
                    provider="google",
                )
            
            except Exception as e:
                # Retry on resource exhaustion / unavailability
                if is_retryable(e) and attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    raise ProviderInvocationError(...)
```

---

## LLMClient Integration

The `LLMClient` uses providers through the strategy pattern:

```python
class LLMClient:
    def __init__(self, budget_limit=None, provider=None):
        # Get provider (auto-detect or explicit)
        if provider is not None:
            self.provider = provider
        else:
            self.provider = get_default_provider()
        
        # Set mode for backward compatibility
        if isinstance(self.provider, NoOpProvider):
            self.mode = "noop"
        else:
            self.mode = self.provider.get_provider_name().lower()
        
        # Initialize safety layers (GAD-509 & GAD-510)
        self.circuit_breaker = CircuitBreaker(...)
        self.quota_manager = OperationalQuota(...)
        self.cost_tracker = CostTracker()
    
    def invoke(self, prompt, model, max_tokens, temperature):
        # 1. Budget check
        if self.budget_limit and self.cost_tracker.total_cost >= self.budget_limit:
            raise BudgetExceededError(...)
        
        # 2. Quota pre-flight check (GAD-510)
        self.quota_manager.check_before_request(
            estimated_tokens=max_tokens,
            operation=f"invoke({model})"
        )
        
        # 3. Provider invocation through circuit breaker (GAD-509)
        provider_response = self.circuit_breaker.call(
            self.provider.invoke,
            prompt=prompt,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        
        # 4. Cost tracking
        usage = self.cost_tracker.record(
            input_tokens=provider_response.usage.input_tokens,
            output_tokens=provider_response.usage.output_tokens,
            model=provider_response.model,
            cost_usd=provider_response.usage.cost_usd,
        )
        
        # 5. Quota recording (GAD-510)
        self.quota_manager.record_request(
            tokens_used=total_tokens,
            cost_usd=usage.cost_usd,
            operation=f"invoke({model})"
        )
        
        return LLMResponse(
            content=provider_response.content,
            usage=usage,
            model=provider_response.model,
            finish_reason=provider_response.finish_reason,
        )
```

---

## Usage Examples

### Example 1: Auto-Detection (Anthropic)

```python
import os
from agency_os.00_system.runtime.llm_client import LLMClient

os.environ["ANTHROPIC_API_KEY"] = "sk-ant-..."

# Auto-detects Anthropic provider
client = LLMClient(budget_limit=10.0)

print(f"Mode: {client.mode}")  # Output: "anthropic"
print(f"Provider: {client.provider.get_provider_name()}")  # Output: "Anthropic"

response = client.invoke(
    prompt="What is machine learning?",
    model="claude-3-5-sonnet-20241022",
    max_tokens=500,
)

print(f"Response: {response.content}")
print(f"Cost: ${response.usage.cost_usd:.4f}")
```

### Example 2: Auto-Detection (Google)

```python
import os
from agency_os.00_system.runtime.llm_client import LLMClient

os.environ["GOOGLE_API_KEY"] = "..."

# Auto-detects Google provider (checked first)
client = LLMClient()

print(f"Mode: {client.mode}")  # Output: "google"
print(f"Provider: {client.provider.get_provider_name()}")  # Output: "Google"

response = client.invoke(
    prompt="Explain photosynthesis",
    model="gemini-2.5-flash-exp",
    max_tokens=1000,
)

print(f"Response: {response.content}")
print(f"Cost: ${response.usage.cost_usd:.4f}")  # Output: $0.0000 (free)
```

### Example 3: Explicit Provider

```python
from agency_os.00_system.runtime.providers import create_provider
from agency_os.00_system.runtime.llm_client import LLMClient
import os

# Explicitly use Anthropic
anthropic_provider = create_provider(
    provider_name="anthropic",
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)

client = LLMClient(provider=anthropic_provider)

response = client.invoke(
    prompt="What is AI?",
    model="claude-3-5-sonnet-20241022",
    max_tokens=200,
)
```

### Example 4: Graceful Fallback (No API Keys)

```python
import os
from agency_os.00_system.runtime.llm_client import LLMClient

# Ensure no API keys
os.environ.pop("ANTHROPIC_API_KEY", None)
os.environ.pop("GOOGLE_API_KEY", None)
os.environ.pop("OPENAI_API_KEY", None)

# Client initializes with NoOp provider (no crash)
client = LLMClient()

print(f"Mode: {client.mode}")  # Output: "noop"
print(f"Provider: {client.provider.get_provider_name()}")  # Output: "NoOp"

response = client.invoke(
    prompt="Test",
    model="noop",
)

print(f"Response: {response.content}")  # Output: "{}"
print(f"Cost: ${response.usage.cost_usd}")  # Output: 0.0
```

### Example 5: Error Handling

```python
from agency_os.00_system.runtime.providers import (
    create_provider,
    ProviderNotAvailableError,
)
from agency_os.00_system.runtime.providers import NoOpProvider

try:
    # Try to create Anthropic provider
    provider = create_provider(
        provider_name="anthropic",
        api_key=None,  # No API key
    )
except ProviderNotAvailableError as e:
    # Fall back to NoOp
    provider = NoOpProvider()
    print(f"Using NoOp provider: {e}")
```

---

## System Architecture

### Complete Layering

```
┌─────────────────────────────────────┐
│     LLMClient (Public API)            │
│     - Budget enforcement              │
│     - Cost tracking                   │
│     - Backward compatibility          │
└────────────┬────────────────────────┘
             │ (delegates to)
             ▼
┌─────────────────────────────────────┐
│   OperationalQuota (GAD-510)          │
│   - Quota enforcement                 │
│   - Pre-flight checks                 │
└────────────┬────────────────────────┘
             │ (wrapped by)
             ▼
┌─────────────────────────────────────┐
│   CircuitBreaker (GAD-509)            │
│   - Cascading failure prevention      │
│   - API degradation protection        │
└────────────┬────────────────────────┘
             │ (delegates to)
             ▼
┌─────────────────────────────────────┐
│     LLMProvider (GAD-511 Strategy)    │
│     ├─ AnthropicProvider              │
│     ├─ GoogleProvider                 │
│     └─ NoOpProvider (fallback)        │
└─────────────────────────────────────┘
```

### System Benefits

1. **Multi-provider flexibility:** Switch via one env var
2. **No crashes:** Mock mode if keys missing
3. **Cost control:** Quotas (GAD-510) prevent overspending
4. **Resilience:** Circuit breaker (GAD-509) protects against degradation
5. **Uniform interface:** All providers expose same API
6. **Cost transparency:** Every call reports actual costs

---

## Testing

### Test Providers Directly

```python
def test_anthropic_provider():
    provider = AnthropicProvider(api_key="sk-ant-...")
    
    response = provider.invoke(
        prompt="Test",
        model="claude-3-5-sonnet-20241022",
        max_tokens=100,
    )
    
    assert response.provider == "anthropic"
    assert response.usage.cost_usd > 0
    assert len(response.content) > 0

def test_noop_provider():
    provider = NoOpProvider()
    
    response = provider.invoke(
        prompt="Test",
        model="noop",
    )
    
    assert response.provider == "noop"
    assert response.usage.cost_usd == 0.0
    assert response.content == "{}"
```

### Test Auto-Detection

```python
import os
from agency_os.00_system.runtime.providers import get_default_provider

def test_auto_detect_google():
    os.environ["GOOGLE_API_KEY"] = "test-key-123"
    os.environ.pop("ANTHROPIC_API_KEY", None)
    
    provider = get_default_provider()
    assert provider.get_provider_name() == "Google"

def test_auto_detect_anthropic():
    os.environ.pop("GOOGLE_API_KEY", None)
    os.environ["ANTHROPIC_API_KEY"] = "sk-ant-..."
    
    provider = get_default_provider()
    assert provider.get_provider_name() == "Anthropic"

def test_auto_detect_noop():
    os.environ.pop("GOOGLE_API_KEY", None)
    os.environ.pop("ANTHROPIC_API_KEY", None)
    os.environ.pop("OPENAI_API_KEY", None)
    
    provider = get_default_provider()
    assert provider.get_provider_name() == "NoOp"
```

---

## Related GADs

- **GAD-509:** Circuit Breaker (protects all providers)
- **GAD-510:** Quota Manager (enforces limits on all providers)

---

## References

- **Implementation:**
  - `agency_os/00_system/runtime/llm_client.py`
  - `agency_os/00_system/runtime/providers/base.py`
  - `agency_os/00_system/runtime/providers/factory.py`
  - `agency_os/00_system/runtime/providers/anthropic.py`
  - `agency_os/00_system/runtime/providers/google.py`

- **Pattern:** Strategy Pattern (GoF Design Patterns)
- **Pricing:** 
  - Anthropic: https://www.anthropic.com/pricing
  - Google: https://ai.google.dev/pricing

---

## Changelog

**v1.0** — Initial implementation
- AnthropicProvider (Claude 3.5 Sonnet)
- GoogleProvider (Gemini 2.5/2.0/1.5)
- NoOpProvider (fallback)
- Factory-based auto-detection
- Graceful failover

**v1.1** (Planned - Phase 2)
- OpenAIProvider (GPT-4, GPT-3.5)
- LocalProvider (Ollama, vLLM)
- Provider load balancing

